{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPdG9w4z/zGmW4KWstb1iD1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GregoryG3/Thesis/blob/main/Initialization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Initialization steps\n",
        "\n"
      ],
      "metadata": {
        "id": "7tMA5FMnbzsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GITHUB"
      ],
      "metadata": {
        "id": "AzoPdyj2d8Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "username = 'GregoryG3'\n",
        "repository = 'Thesis'\n",
        "git_token = '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'"
      ],
      "metadata": {
        "id": "1zrX10zad7-j"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://{git_token}@github.com/{username}/{repository}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIUJigFPez_9",
        "outputId": "184b089e-8a41-4180-f968-3b34d4ca8052"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Thesis'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/17)\u001b[K\rremote: Counting objects:  11% (2/17)\u001b[K\rremote: Counting objects:  17% (3/17)\u001b[K\rremote: Counting objects:  23% (4/17)\u001b[K\rremote: Counting objects:  29% (5/17)\u001b[K\rremote: Counting objects:  35% (6/17)\u001b[K\rremote: Counting objects:  41% (7/17)\u001b[K\rremote: Counting objects:  47% (8/17)\u001b[K\rremote: Counting objects:  52% (9/17)\u001b[K\rremote: Counting objects:  58% (10/17)\u001b[K\rremote: Counting objects:  64% (11/17)\u001b[K\rremote: Counting objects:  70% (12/17)\u001b[K\rremote: Counting objects:  76% (13/17)\u001b[K\rremote: Counting objects:  82% (14/17)\u001b[K\rremote: Counting objects:  88% (15/17)\u001b[K\rremote: Counting objects:  94% (16/17)\u001b[K\rremote: Counting objects: 100% (17/17)\u001b[K\rremote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 17 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (17/17), 1.89 MiB | 7.24 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {repository}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF0FmH2Xe8QF",
        "outputId": "01225920-9b44-43cb-86f2-b821dce40fe5"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Thesis/Thesis/Thesis/Thesis/Thesis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arzhZcOOfC6n",
        "outputId": "d2df876f-ef52-4417-ae4a-d08c06399ba9"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[01;34m..\u001b[0m/  EDA_wstepna_analiza.ipynb  \u001b[01;34m.git\u001b[0m/  Initialization.ipynb  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commends:"
      ],
      "metadata": {
        "id": "qdbW_vSkfPis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWU9xyeFfNSL",
        "outputId": "4475fbb4-e228-4ad5-804c-8a8152ed9fbb"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"grzegorz.grzelak19@gmail.com\"\n",
        "!git config --global user.name \"GregoryG3\"\n"
      ],
      "metadata": {
        "id": "NzplzeEh0F72"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add Initialization.ipynb"
      ],
      "metadata": {
        "id": "3or3ZiSu0pCv"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -a -m \"Merging data in full dataframe and ratings full dataframe\""
      ],
      "metadata": {
        "id": "pAYXbDuHfRIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d04354b-282c-4c0e-8ac2-ac1e58fe74c0"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you are satisfied with your work and want to save it, you can simply push your commits to GitHub"
      ],
      "metadata": {
        "id": "FqI0h97Ffbc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git push"
      ],
      "metadata": {
        "id": "Zm_0JjCtfXQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e44b58f-eb25-4520-f45a-9af8eee24053"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Password for 'https://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@github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "Zdz0zKEGbxdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "from os.path import join\n"
      ],
      "metadata": {
        "id": "eBdagiGk7leK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Google drive"
      ],
      "metadata": {
        "id": "VyantTM9cV0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWXoIpLX9QSu",
        "outputId": "2b08022e-b3d6-492c-e1c7-fefceed112d6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "yuapH-tJ_1lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_DIR = \"/content/drive/MyDrive/Thesis/Dataset\"\n",
        "RAW_DIR = join(PROJECT_DIR, \"raw\")\n",
        "PROCESSED_DIR = join(PROJECT_DIR, \"processed\")"
      ],
      "metadata": {
        "id": "kMQAEWWG7mGl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_columns_total = [\"bikeReasons\", \"vehiclesOwned\",\n",
        "                     \"whyBiking\", \"introSelection\"]\n",
        "dict_columns_total = [\"berlinTraffic\",\n",
        "                     \"motivationalFactors\",\n",
        "                     \"transportRatings\",\n",
        "                     \"responsible\",\n",
        "                     \"climateTraffic\",\n",
        "                     \"sharingConditions\",\n",
        "                     \"sharingModes\",\n",
        "                     \"saveSpace\",\n",
        "                     \"annoyingPeople\"]"
      ],
      "metadata": {
        "id": "aPDk60ueB6M_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(filename):\n",
        "    \"\"\"Load raw data file and create dataframes for the ratings and the profiles\n",
        "        Arguments:\n",
        "        filename {str} -- filename of the raw data used\n",
        "    \"\"\"\n",
        "\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info('Preprocessing data')\n",
        "    data = json.load(open(join(RAW_DIR, filename), \"rb\"))\n",
        "    profiles_list = []\n",
        "    data_df = pd.DataFrame()\n",
        "    logger.info('Raw data contains %s entries', len(data))\n",
        "\n",
        "    # Spliting to 2 data frame - ratings and profiles\n",
        "    for data_point in data:\n",
        "        rating_df = pd.DataFrame(data_point[\"ratings\"])\n",
        "        rating_df[\"sessionID\"] = data_point[\"profile\"][\"sessionID\"]\n",
        "        rating_df = rating_df.reset_index(level=0)\n",
        "        data_df = pd.concat([data_df, rating_df], ignore_index=True, sort=False)\n",
        "        profiles_list.append(data_point[\"profile\"])\n",
        "    profiles_df = pd.DataFrame(profiles_list).set_index(\"sessionID\")\n",
        "\n",
        "    logger.info(\"Drop 'isTosAccepted' column\")\n",
        "    # Drop columns - I checked that everybody accpet the Privacy Policy\n",
        "    columns_to_drop = [\"isTosAccepted\"] # \"berlinTraffic_0\"\n",
        "    profiles_df = profiles_df.drop(columns_to_drop, axis=1)\n",
        "\n",
        "\n",
        "    dict_columns = [x for x in dict_columns_total if x in profiles_df.columns]\n",
        "    list_columns = [x for x in list_columns_total if x in profiles_df.columns]\n",
        "    logger.info(\"Split dictionary-type columns into separate columns\")\n",
        "    # iterates through dictionary-type columns splitting each into separate columns\n",
        "    for col in dict_columns:\n",
        "        new_columns = profiles_df[col].apply(pd.Series)\n",
        "        new_columns.columns = [col + \"_\" + str(name)\n",
        "                                for name in new_columns.columns]\n",
        "        profiles_df = pd.concat([profiles_df.drop([col], axis=1),\n",
        "                                  new_columns],\n",
        "                                axis=1)\n",
        "    logger.info(\"Split list-type columns into separate columns\")\n",
        "    # iterates through list-type columns splitting each into separate columns\n",
        "    for col in list_columns:\n",
        "        for subject in profiles_df.index:\n",
        "            answers = profiles_df.loc[subject][col]\n",
        "            counts = pd.Series(\n",
        "                      answers,\n",
        "                      dtype=\"object\").value_counts().rename(\n",
        "                                                    subject).add_prefix(\n",
        "                                                              col + \"_\")\n",
        "            for i in counts.index:\n",
        "                if i not in profiles_df.columns:\n",
        "                    profiles_df[i] = np.nan\n",
        "            copy = profiles_df.loc[subject].copy()\n",
        "            copy[counts.index] = counts\n",
        "            profiles_df.loc[subject] = copy\n",
        "        profiles_df = profiles_df.drop(col, axis=1)\n",
        "\n",
        "    logger.info(\"Save to csv\")\n",
        "    profiles_df.to_csv(join(PROCESSED_DIR, \"profiles_df_without_state.csv\"))\n",
        "    data_df.to_csv(join(PROCESSED_DIR, \"ratings.csv\"), index=False)\n"
      ],
      "metadata": {
        "id": "A3BWNDHnvff3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\" Runs data processing scripts to turn raw data from (../raw) into\n",
        "        cleaned data ready to be analyzed (saved in ../processed).\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    logger.info('Making final data set from raw data')\n",
        "\n",
        "    filename = \"SurveyResults_200414.json\"\n",
        "    preprocessing(filename)"
      ],
      "metadata": {
        "id": "RmqXJMzRwaSU"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run main preprocessing:"
      ],
      "metadata": {
        "id": "0aHBbXcW4Juu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "KKT1Jy0Aw8Nq"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert zipcode to specific district (only Berlin) and state"
      ],
      "metadata": {
        "id": "RX0REIgi4WTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zipcode_convert():\n",
        "    profiles_df = pd.read_csv(join(PROCESSED_DIR, \"profiles_df_without_state.csv\"),\n",
        "                            low_memory=False)\n",
        "    plz2district = pd.read_csv(join(PROJECT_DIR, \"external/Postcode_BerlinDistricts.csv\"),\n",
        "                            index_col=0, sep=';')\n",
        "    germany_states = pd.read_csv(join(PROJECT_DIR, \"external/Germany_complete_gis_info.csv\"),\n",
        "                            index_col=0, sep=';')\n",
        "\n",
        "    zipcode_to_district = plz2district['Stadtteil'].to_dict()\n",
        "    profiles_df['district'] = profiles_df['zipcode'].map(zipcode_to_district)\n",
        "\n",
        "    zipcode_to_state = germany_states['State'].to_dict()\n",
        "    district_col_index = profiles_df.columns.get_loc('district')\n",
        "    profiles_df.insert(district_col_index + 1, 'state', profiles_df['zipcode'].map(zipcode_to_state))\n",
        "\n",
        "    profiles_df.to_csv(join(PROCESSED_DIR, \"profiles_df.csv\"))"
      ],
      "metadata": {
        "id": "0S5CtfbGBIxh"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    zipcode_convert()"
      ],
      "metadata": {
        "id": "A9mVsGJsjh4t"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging profiles and ratings to a single dataframe with the parameters of the scenes"
      ],
      "metadata": {
        "id": "UuB2t76Dmp0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merging():\n",
        "    \"\"\"Merge subject profiles and ratings to a single dataframe with\n",
        "       the parameters of the scenes\n",
        "    \"\"\"\n",
        "    profiles_df = pd.read_csv(join(PROCESSED_DIR, \"profiles_df.csv\"),\n",
        "                              low_memory=False)\n",
        "    ratings_df = pd.read_csv(join(PROCESSED_DIR, \"ratings.csv\"), low_memory=False)\n",
        "\n",
        "    #load dataframes about specific situations:\n",
        "    rename = {\"SceneID\": \"scene_id\"}\n",
        "    scenes_cp = pd.read_csv(\n",
        "                join(RAW_DIR,\n",
        "                     \"scenes_cp.csv\")).rename(columns=rename).drop([\"weight\",\n",
        "                                                                    \"Raus\",\n",
        "                                                                    \"SR_lD\"],\n",
        "                                                                   axis=1)\n",
        "    scenes_ms = pd.read_csv(\n",
        "                join(RAW_DIR,\n",
        "                     \"scenes_ms.csv\")).rename(columns=rename).drop([\"weight\",\n",
        "                                                                    \"HVS_lD\",\n",
        "                                                                    \"Raus\"],\n",
        "                                                                   axis=1)\n",
        "    scenes_se = pd.read_csv(\n",
        "                join(RAW_DIR,\n",
        "                     \"scenes_se.csv\")).rename(columns=rename).drop([\"weight\",\n",
        "                                                                    \"NVS_lD\",\n",
        "                                                                    \"raus\"],\n",
        "                                                                   axis=1)\n",
        "\n",
        "    cp_df = pd.merge(ratings_df, scenes_cp, on=\"scene_id\")\n",
        "    ms_df = pd.merge(ratings_df, scenes_ms, on=\"scene_id\")\n",
        "    se_df = pd.merge(ratings_df, scenes_se, on=\"scene_id\")\n",
        "\n",
        "    #dataset witj ratings and scenes information\n",
        "    ratings_full_df = pd.concat([cp_df, ms_df, se_df], sort=False)\n",
        "    #dataset with ratings, profile and scenes information\n",
        "    full_df = pd.merge(ratings_full_df, profiles_df, how=\"left\", on=\"sessionID\")\n",
        "\n",
        "    full_df.to_csv(join(PROCESSED_DIR, \"full_df.csv\"), index=False)\n",
        "    ratings_full_df.to_csv(join(PROCESSED_DIR, \"ratings_df.csv\"), index=False)\n"
      ],
      "metadata": {
        "id": "6pRXjP-Hmn7b"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    merging()"
      ],
      "metadata": {
        "id": "jb9bvhIevall"
      },
      "execution_count": 142,
      "outputs": []
    }
  ]
}